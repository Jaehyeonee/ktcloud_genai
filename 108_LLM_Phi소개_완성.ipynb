{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
    },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Phi 소개**"
      ],
      "metadata": {
        "id": "xAPO58qbDYi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NAmo5uhXQ46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요."
      ],
      "metadata": {
        "id": "KHIcfXn8RArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K-OX9xcYMp_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.**Phi란?**"
      ],
      "metadata": {
        "id": "QD5DBVW269sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **Microsoft가 개발한 소형 언어 모델(SLM) 시리즈**\n",
        "    - 최근 연구에서는 Phi-2 (2.7B 파라미터), **Phi-3 시리즈** (최대 14B 파라미터)가 공개되었고, 성능 대비 크기가 작아 소형 GPU·노트북·모바일 기기에서도 활용 가능한 것이 특징\n",
        "    - Phi‑3‑mini (3.8B 파라미터), Phi‑3‑small (약 7B), Phi‑3‑medium (14B) 등 다양한 변형을 포함\n",
        "    \n",
        "- **파라미터 & 컨텍스트 윈도우**\n",
        "    - Phi-3-mini: 3.8B 파라미터, 4K 및 확장된 128K 컨텍스트 윈도우 지원\n",
        "    - Phi‑3‑small: 약 7B, 기본 8K 컨텍스트\n",
        "    - Phi‑3‑medium: 14B 기능, 더 넓은 컨텍스트 처리 능력\n",
        "- **학습 데이터 & 미세조정**\n",
        "    - 총 3.3조 토큰 이상으로 구성된 고품질 필터링 웹 데이터, 합성 데이터 등을 활용. 이후 **Supervised Fine-Tuning (SFT)**과 Direct Preference Optimization (DPO) 기법을 통해 인간 선호도 및 안전 기준에 맞춰 미세 조정됨\n",
        "- **성능 지표**\n",
        "    - 최소형 모델(Phi-3-mini)도 MMLU 69%, MT-bench 8.38 등 성능으로 Mixtral 8x7B, GPT-3.5와 유사한 수준\n",
        "    - Phi‑3‑small 및 medium는 각각 MMLU 75% 및 78%, MT‑bench 8.7 및 8.9를 달성\n",
        "    - 여러 벤치마크(HellaSwag, WinoGrande, TruthfulQA, HumanEval 등)에서 강력한 성능을 보임\n",
        "- **이용 가능 플랫폼 및 최적화**\n",
        "    - Azure AI Studio, Hugging Face, Ollama에서 사용 가능\n",
        "    - ONNX Runtime과 Windows DirectML을 통한 GPU/CPU 및 모바일 기기 최적화 지원\n",
        "    - Ollama를 통해 로컬에서도 실행 가능하며, 긴 문맥 처리가 특징\n",
        "- **확장 및 응용성**\n",
        "    - 리소스 제약 환경, 저지연 요구 시나리오, 강력한 추론/수학/로직 처리, 긴 컨텍스트 필요 작업에 적합\n",
        "    - Strathweb Phi Engine처럼 다양한 플랫폼(C#, Swift, Kotlin 등)에서 로컬 실행을 쉽게 해 주는 라이브러리도 존재\n",
        "- **책임있는 AI 설계**\n",
        "    - Microsoft의 Responsible AI 기준에 따라 설계됨. RLHF, 자동화 테스트, red-teaming 등을 포함한 안전성 검증 프로세스가 적용됨\n"
      ],
      "metadata": {
        "id": "olZXCZdcWh_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.Phi 사용방식**\n"
      ],
      "metadata": {
        "id": "atcqEW7t35zB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXp09JFsFBXi"
      },
      "source": [
        "첫 번째 단계로 빠른 추론을 위해 모델을 GPU에 로드합니다. 모델과 토크나이저를 각각 로드합니다(항상 이렇게 할 필요는 없습니다)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **모델 로드 및 토큰 생성**\n"
      ],
      "metadata": {
        "id": "QZO8os_a35zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **중간수준 API(Task-specific Models) 방식**\n",
        "(코랩(무료)에서 실행 오래 걸림(5분⭡))"
      ],
      "metadata": {
        "id": "Pfj85umD35zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IJ-8XwiXeeRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.48.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG7Mcwrtk04h",
        "outputId": "e499e1db-b66a-4582-a049-8561b9904692"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.48.3 in /usr/local/lib/python3.12/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-3-mini 로드 및 간단 생성 (Transformers)\n",
        "#   선형적 생성 흐름을 체험하며 모델 작동 원리를 설명하기 좋다.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 모델과 토크나이저를 로드합니다.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n"
      ],
      "metadata": {
        "id": "VJE9wAOw35zC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "c737f7bb0b0e4a049772ffe6d599d46e",
            "947982156cb64122943661d8ae6e6657",
            "c15a328b83e24969bbfa4ce1181cd25e",
            "afd73dc60f9c4bb5b97ac0405c379e6d",
            "25c12f0273e44cc1abe9dd71e9246e18",
            "7cc5eca5941e406b9059c33148b375f6",
            "11a7d50e619d47feacacf9278d2831f8",
            "03dc17ee1dc84914bb8db17b5b68a7f9",
            "22849804229343a9a466fc9926b36fa4",
            "3721d67b91b640ceb3a2d880d66e94ca",
            "424e7d245e6b429a93a492a3bc00cfbd"
          ]
        },
        "outputId": "c13f0fd8-820e-49b8-8114-35d00ddb3728"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c737f7bb0b0e4a049772ffe6d599d46e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **고수준 API(pipeline) 방식**\n",
        "\n",
        "모델과 토크나이저를 직접 사용할 수 있지만 pipeline 함수를 사용하는 것이 훨씬 쉽습니다."
      ],
      "metadata": {
        "id": "MGFh99qE35zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 파이프라인을 만듭니다.\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "id": "jOOq78Rx35zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab96990-e6d0-4a2e-e161-eb4224675a09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 💡 **[참고] PyTorch의 언어 모델 생성 함수에서 사용되는 주요 매개변수**\n",
        "\n",
        "| 매개변수 | 의미 | 기본값 | 범위 | 설명 |\n",
        "|--- |--- |--- |--- |--- |\n",
        "| **max_new_tokens**  |모델이 생성할 새로운 토큰의 최대 개수  |모델마다 다름 (보통 50-100)  |1 ~ 모델의 최대 컨텍스트 길이  |  |\n",
        "|**temperature**  |모델 출력의 창의성과 일관성을 조절하는 매개변수  |1.0  |0.0 ~ 2.0 (일반적으로 0.1 ~ 1.5)  |Softmax 함수의 온도 매개변수  |\n",
        "|**do_sample**  |샘플링 방식 선택 (확률적 vs 결정적)  |False  |Boolean (True/False)  |  |"
      ],
      "metadata": {
        "id": "4clCIvrf_qbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------\n",
        "# max_new_tokens\n",
        "#--------------------------\n",
        "# 짧은 답변 (요약, 분류)\n",
        "max_new_tokens=50\n",
        "\n",
        "# 중간 길이 답변 (설명, Q&A)\n",
        "max_new_tokens=200\n",
        "\n",
        "# 긴 답변 (에세이, 보고서)\n",
        "max_new_tokens=500"
      ],
      "metadata": {
        "id": "01YVt_XCA6MQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Temperature** 값에 따른 특성\n",
        "\n",
        "|Temperature|특성|사용 상황|예시|\n",
        "|--- |--- |--- |---|\n",
        "|0.0 | 완전히 결정적, 항상 같은 출력 | 정확한 답변 필요 | 수학 문제, 번역 |\n",
        "|0.3-0.5 | 안정적, 일관성 높음 | 비즈니스 문서, 기술 설명 | 보고서 작성 |\n",
        "|0.7 | 균형 잡힌 창의성과 일관성 | 일반적인 대화, 교육 | 챗봇, Q&A |\n",
        "|1.0 | 표준 창의성 | 기본 설정 | 기본 대화\n",
        "|1.5-2.0 | 매우 창의적, 예측 불가 | 창작 활동, 브레인스토밍 | 소설, 시 창작 |"
      ],
      "metadata": {
        "id": "36N_NtELBcBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------\n",
        "# Temperature 작동 원리\n",
        "#--------------------------\n",
        "# 수학적 표현\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def apply_temperature(logits, temperature):\n",
        "    \"\"\"Temperature scaling 적용\"\"\"\n",
        "    if temperature == 0:\n",
        "        return torch.argmax(logits, dim=-1)\n",
        "    else:\n",
        "        scaled_logits = logits / temperature\n",
        "        probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "        return torch.multinomial(probabilities, 1)\n",
        "\n",
        "# 예시: 다음 단어 예측 확률\n",
        "original_probs = [0.6, 0.3, 0.1]  # \"좋은\", \"나쁜\", \"보통\"\n",
        "\n",
        "# Temperature = 0.5 (더 확실한 선택)\n",
        "temp_05_probs = [0.8, 0.15, 0.05]\n",
        "\n",
        "# Temperature = 1.5 (더 다양한 선택)\n",
        "temp_15_probs = [0.45, 0.35, 0.2]"
      ],
      "metadata": {
        "id": "kL8K-xSIBQUi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **do_sample**옵션 비교\n",
        "\n",
        "| do_sample |  방식 | 특징 | 언제 사용? |\n",
        "|--- |--- |--- |--- |\n",
        "| False | Greedy Decoding | 항상 가장 확률 높은 토큰 선택 | 정확성이 중요한 작업 |\n",
        "| True | Sampling | 확률에 따라 랜덤하게 선택 | 창의성이 필요한 작업 |"
      ],
      "metadata": {
        "id": "j9P_SQq__0Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# do_sample=False (Greedy)\n",
        "다음 단어 확률: {\"좋은\": 0.6, \"나쁜\": 0.3, \"보통\": 0.1}\n",
        "선택: 항상 \"좋은\" (가장 높은 확률)\n",
        "\n",
        "# do_sample=True (Sampling)\n",
        "다음 단어 확률: {\"좋은\": 0.6, \"나쁜\": 0.3, \"보통\": 0.1}\n",
        "선택: 60% 확률로 \"좋은\", 30% 확률로 \"나쁜\", 10% 확률로 \"보통\""
      ],
      "metadata": {
        "id": "dj0OspUlrN1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **프롬프트 작성 및 질의**"
      ],
      "metadata": {
        "id": "wbFuicMk35zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 프롬프트 (사용자 입력 / 쿼리)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# 출력 생성\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "9G-g_BGk35zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f49df8f-72f8-4193-f349-4f51d3f13f04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Why did the chicken join the band? Because it had the drumsticks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.예제로 QuickTour**"
      ],
      "metadata": {
        "id": "JFOVRwVI8bjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: 기본 텍스트 생성**"
      ],
      "metadata": {
        "id": "hLTlimBYRfid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # GPU 메모리 절약\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 채팅 형식 프롬프트 구성\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"please explain the difference between tuple and dictionary.\"}\n",
        "]\n",
        "\n",
        "# 토큰화 및 생성\n",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "# input_ids = input_ids.to('cuda')\n",
        "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "# 텍스트 생성\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,  #모델이 생성할 새로운 토큰의 최대 개수\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "# 결과 출력\n",
        "print('-' * 50)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1z-FreQiRlsQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "52096082425e494886093f0852f88de3",
            "afa5b5fac97f41eeb83dc44d59f08065",
            "4a6ff28efd3b4d5487365ff26364e4ae",
            "4ea9fa7331ea4697a4ebac008992c6b5",
            "e7112bf7f9a94a88be1331a7b0493df3",
            "5fab4049de494f3fa61177e137d61893",
            "e5b6c17f3ff84ba199a889c787211f45",
            "2ee83b8b0bf14abca6a7472028826dae",
            "29edce5e1db94cefa601a600b995fbb9",
            "8b0f99e99e414524a6e93b8faa5e0ab4",
            "c426c26e03734b449eec5aec122ea2d2"
          ]
        },
        "outputId": "a99668ce-6c5f-42bd-ae63-4d056490e1a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52096082425e494886093f0852f88de3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "You are a helpful AI assistant. please explain the difference between tuple and dictionary. \n",
            " I need to create a C# class that models an error object for a data service. It should inherit from a base error class, have properties for error code (string), error message (string), error type (string), and a nullable `DateTime?` for when the error occurred. Please provide XML documentation for each property, and include constructors: a parameterless one, and another that takes an error code, message, type, and occurrence time. Also, ensure it's clear that the class is auto-generated and shouldn't be manually altered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: 코딩 어시스턴트**"
      ],
      "metadata": {
        "id": "uRwt3o1zSpV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coding_assistant_example():\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"당신은 파이썬 프로그래밍 전문가입니다.\"},\n",
        "        {\"role\": \"user\", \"content\": \"피보나치 수열을 계산하는 효율적인 함수를 작성해주세요.\"}\n",
        "    ]\n",
        "\n",
        "    # 파이프라인 생성\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # 응답 생성\n",
        "    print('-' * 50)\n",
        "    response = pipe(messages)\n",
        "    print(response[0]['generated_text'])\n",
        "coding_assistant_example()"
      ],
      "metadata": {
        "id": "Bgv8CP_rSw6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa140b9-7759-425c-f7cb-3f71e36a3471"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "[{'role': 'system', 'content': '당신은 파이썬 프로그래밍 전문가입니다.'}, {'role': 'user', 'content': '피보나치 수열을 계산하는 효율적인 함수를 작성해주세요.'}, {'role': 'assistant', 'content': ' 피보나치 수열은 첫 번째 두 개의 수가 1이고 두 번째 번째 수는 2이며, 그 이후 각 수는 이전 두 수의 합으로 계산됩니다. 다음은 이 함수를 파이썬으로 구현한 것입니다:\\n\\n\\n```python\\n\\ndef fibonacci(n):\\n\\n    if n <= 0:\\n\\n        return []\\n\\n    elif n == 1:\\n\\n        return [0]\\n\\n    elif n == 2:\\n\\n        return [0, 1]\\n\\n\\n    fib_sequence = [0, 1]\\n\\n    for i in range(2, n):\\n\\n        next_number = fib_sequence[i-1] + fib_sequence[i-2]\\n\\n        fib_sequence.append(next_number)\\n\\n    return fib_sequence\\n\\n\\n# 피보나치 수열의 '}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: 교육용 Q&A 시스템**"
      ],
      "metadata": {
        "id": "9qDx6AzEShCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def educational_qa_system():\n",
        "    \"\"\"교육용 질의응답 시스템\"\"\"\n",
        "\n",
        "    class Phi3EducationBot:\n",
        "        def __init__(self, model_name=\"microsoft/Phi-3-mini-4k-instruct\"):\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            self.pipe = pipeline(\"text-generation\",\n",
        "                               model=self.model,\n",
        "                               tokenizer=self.tokenizer)\n",
        "\n",
        "        def answer_question(self, question, subject=\"컴퓨터공학\"):\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"당신은 {subject} 전문 강사입니다. 학생들이 이해하기 쉽게 설명해주세요.\"},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ]\n",
        "\n",
        "            response = self.pipe(messages, max_new_tokens=250)\n",
        "            return response[0]['generated_text']\n",
        "\n",
        "    # 사용 예시\n",
        "    bot = Phi3EducationBot()\n",
        "    answer = bot.answer_question(\"머신러닝과 딥러닝의 차이점은 무엇인가요?\")\n",
        "    print(answer)\n",
        "\n",
        "educational_qa_system()"
      ],
      "metadata": {
        "id": "d7m4784iSMhd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9a9f286db85041fb828573923855240f",
            "7c634f2001a54be4a0976f78ee27d6f9",
            "4ece9c9126e34cf28c70e80da21539ba",
            "1758babdd46a4aad8446ac130eb17824",
            "dbacab70e2e14cdab88437b27ad3bb91",
            "98a8e5f0f3e24f9bb46245dc674f2ff2",
            "595d8354af364b11845361b43022ad17",
            "e6895c046c424c2496c9c3d72bc99189",
            "0dc8e6ee3b0547e7b95c8c9c87b2e168",
            "dd4417144f094dc18178e8b06b266173",
            "7f29356ba5774ebfafe92626651922f3"
          ]
        },
        "outputId": "397f537b-5273-44dc-9015-44cd28d41a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a9f286db85041fb828573923855240f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: 수학적 추론 테스트**"
      ],
      "metadata": {
        "id": "-x5CFg5mYDhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 수학적 추론 테스트\n",
        "#   직접 알고리즘 사고가 나오는 과정을 볼 수 있어 교육적으로 유익(?)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "\n",
        "# 프롬프트 (사용자 입력 / 쿼리)\n",
        "prompt = \"시곗바늘이 15분 동안 얼마나 움직일까요?\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "c32AwHKIYmPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVmM3W2w62-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.모델 컨텍스트 윈도우 비교**"
      ],
      "metadata": {
        "id": "-Zj0GKg86tq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: Phi-3 4K vs 128K 컨텍스트 비교 실습**\n",
        "**[주의!]** 모델 크기가 크기 때문에 128K 모델은 실행 속도가 느릴 수 있다."
      ],
      "metadata": {
        "id": "IezMGMJIY6Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**실험 포인트**\n",
        "\n",
        "1. 4K 버전 (Phi-3-mini-4k)\n",
        "    - 4,096 토큰까지만 인식\n",
        "    - 긴 문맥(30K 토큰)을 넣으면 앞부분은 잘라버림 → “정보 손실” 발생\n",
        "    - 따라서 답변이 틀리거나 불완전할 수 있음\n",
        "2. 128K 버전 (Phi-3-mini-128k)\n",
        "    - 최대 128,000 토큰까지 인식\n",
        "    - 긴 문맥 전체를 유지\n",
        "    - 답변에서 정확한 문장 개수나 더 깊은 문맥 이해가 가능"
      ],
      "metadata": {
        "id": "KAmWuompZN_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 4K 버전과 128K 버전 각각 불러오기\n",
        "models = {\n",
        "    \"4k\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    \"128k\": \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "}\n",
        "\n",
        "tokenizers = {}\n",
        "loaded_models = {}\n",
        "\n",
        "for key, model_name in models.items():\n",
        "    tokenizers[key] = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    loaded_models[key] = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, trust_remote_code=True, device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "# 긴 텍스트 생성 (테스트용 문맥)\n",
        "long_text = \"이 문장은 테스트 문장입니다. \" * 3000  # 약 30K 토큰 수준\n",
        "\n",
        "prompt = long_text + \"\\n\\n질문: 위 문장의 개수는 몇 개인가요?\"\n",
        "\n",
        "# 두 모델에 각각 입력\n",
        "for key in [\"4k\", \"128k\"]:\n",
        "    tokenizer = tokenizers[key]\n",
        "    model = loaded_models[key]\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    print(f\"\\n===== Phi-3 {key.upper()} 결과 =====\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "wgA_FG5IZFYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0g36MizoMox8"
      }
    }
  ]
}