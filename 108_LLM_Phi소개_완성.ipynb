{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
    },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Phi ì†Œê°œ**"
      ],
      "metadata": {
        "id": "xAPO58qbDYi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NAmo5uhXQ46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ’¡ **NOTE**\n",
        "    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "KHIcfXn8RArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K-OX9xcYMp_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.**Phië€?**"
      ],
      "metadata": {
        "id": "QD5DBVW269sN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- **Microsoftê°€ ê°œë°œí•œ ì†Œí˜• ì–¸ì–´ ëª¨ë¸(SLM) ì‹œë¦¬ì¦ˆ**\n",
        "    - ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” Phi-2 (2.7B íŒŒë¼ë¯¸í„°), **Phi-3 ì‹œë¦¬ì¦ˆ** (ìµœëŒ€ 14B íŒŒë¼ë¯¸í„°)ê°€ ê³µê°œë˜ì—ˆê³ , ì„±ëŠ¥ ëŒ€ë¹„ í¬ê¸°ê°€ ì‘ì•„ ì†Œí˜• GPUÂ·ë…¸íŠ¸ë¶Â·ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œë„ í™œìš© ê°€ëŠ¥í•œ ê²ƒì´ íŠ¹ì§•\n",
        "    - Phiâ€‘3â€‘mini (3.8B íŒŒë¼ë¯¸í„°), Phiâ€‘3â€‘small (ì•½ 7B), Phiâ€‘3â€‘medium (14B) ë“± ë‹¤ì–‘í•œ ë³€í˜•ì„ í¬í•¨\n",
        "    \n",
        "- **íŒŒë¼ë¯¸í„° & ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°**\n",
        "    - Phi-3-mini: 3.8B íŒŒë¼ë¯¸í„°, 4K ë° í™•ì¥ëœ 128K ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì§€ì›\n",
        "    - Phiâ€‘3â€‘small: ì•½ 7B, ê¸°ë³¸ 8K ì»¨í…ìŠ¤íŠ¸\n",
        "    - Phiâ€‘3â€‘medium: 14B ê¸°ëŠ¥, ë” ë„“ì€ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëŠ¥ë ¥\n",
        "- **í•™ìŠµ ë°ì´í„° & ë¯¸ì„¸ì¡°ì •**\n",
        "    - ì´ 3.3ì¡° í† í° ì´ìƒìœ¼ë¡œ êµ¬ì„±ëœ ê³ í’ˆì§ˆ í•„í„°ë§ ì›¹ ë°ì´í„°, í•©ì„± ë°ì´í„° ë“±ì„ í™œìš©. ì´í›„ **Supervised Fine-Tuning (SFT)**ê³¼ Direct Preference Optimization (DPO) ê¸°ë²•ì„ í†µí•´ ì¸ê°„ ì„ í˜¸ë„ ë° ì•ˆì „ ê¸°ì¤€ì— ë§ì¶° ë¯¸ì„¸ ì¡°ì •ë¨\n",
        "- **ì„±ëŠ¥ ì§€í‘œ**\n",
        "    - ìµœì†Œí˜• ëª¨ë¸(Phi-3-mini)ë„ MMLU 69%, MT-bench 8.38 ë“± ì„±ëŠ¥ìœ¼ë¡œ Mixtral 8x7B, GPT-3.5ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€\n",
        "    - Phiâ€‘3â€‘small ë° mediumëŠ” ê°ê° MMLU 75% ë° 78%, MTâ€‘bench 8.7 ë° 8.9ë¥¼ ë‹¬ì„±\n",
        "    - ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬(HellaSwag, WinoGrande, TruthfulQA, HumanEval ë“±)ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì„\n",
        "- **ì´ìš© ê°€ëŠ¥ í”Œë«í¼ ë° ìµœì í™”**\n",
        "    - Azure AI Studio, Hugging Face, Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n",
        "    - ONNX Runtimeê³¼ Windows DirectMLì„ í†µí•œ GPU/CPU ë° ëª¨ë°”ì¼ ê¸°ê¸° ìµœì í™” ì§€ì›\n",
        "    - Ollamaë¥¼ í†µí•´ ë¡œì»¬ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ê°€ íŠ¹ì§•\n",
        "- **í™•ì¥ ë° ì‘ìš©ì„±**\n",
        "    - ë¦¬ì†ŒìŠ¤ ì œì•½ í™˜ê²½, ì €ì§€ì—° ìš”êµ¬ ì‹œë‚˜ë¦¬ì˜¤, ê°•ë ¥í•œ ì¶”ë¡ /ìˆ˜í•™/ë¡œì§ ì²˜ë¦¬, ê¸´ ì»¨í…ìŠ¤íŠ¸ í•„ìš” ì‘ì—…ì— ì í•©\n",
        "    - Strathweb Phi Engineì²˜ëŸ¼ ë‹¤ì–‘í•œ í”Œë«í¼(C#, Swift, Kotlin ë“±)ì—ì„œ ë¡œì»¬ ì‹¤í–‰ì„ ì‰½ê²Œ í•´ ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ì¡´ì¬\n",
        "- **ì±…ì„ìˆëŠ” AI ì„¤ê³„**\n",
        "    - Microsoftì˜ Responsible AI ê¸°ì¤€ì— ë”°ë¼ ì„¤ê³„ë¨. RLHF, ìë™í™” í…ŒìŠ¤íŠ¸, red-teaming ë“±ì„ í¬í•¨í•œ ì•ˆì „ì„± ê²€ì¦ í”„ë¡œì„¸ìŠ¤ê°€ ì ìš©ë¨\n"
      ],
      "metadata": {
        "id": "olZXCZdcWh_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.Phi ì‚¬ìš©ë°©ì‹**\n"
      ],
      "metadata": {
        "id": "atcqEW7t35zB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXp09JFsFBXi"
      },
      "source": [
        "ì²« ë²ˆì§¸ ë‹¨ê³„ë¡œ ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ ëª¨ë¸ì„ GPUì— ë¡œë“œí•©ë‹ˆë‹¤. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ê°ê° ë¡œë“œí•©ë‹ˆë‹¤(í•­ìƒ ì´ë ‡ê²Œ í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ëª¨ë¸ ë¡œë“œ ë° í† í° ìƒì„±**\n"
      ],
      "metadata": {
        "id": "QZO8os_a35zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ì¤‘ê°„ìˆ˜ì¤€ API(Task-specific Models) ë°©ì‹**\n",
        "(ì½”ë©(ë¬´ë£Œ)ì—ì„œ ì‹¤í–‰ ì˜¤ë˜ ê±¸ë¦¼(5ë¶„â­¡))"
      ],
      "metadata": {
        "id": "Pfj85umD35zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IJ-8XwiXeeRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.48.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG7Mcwrtk04h",
        "outputId": "e499e1db-b66a-4582-a049-8561b9904692"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.48.3 in /usr/local/lib/python3.12/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-3-mini ë¡œë“œ ë° ê°„ë‹¨ ìƒì„± (Transformers)\n",
        "#   ì„ í˜•ì  ìƒì„± íë¦„ì„ ì²´í—˜í•˜ë©° ëª¨ë¸ ì‘ë™ ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ê¸° ì¢‹ë‹¤.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n"
      ],
      "metadata": {
        "id": "VJE9wAOw35zC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "c737f7bb0b0e4a049772ffe6d599d46e",
            "947982156cb64122943661d8ae6e6657",
            "c15a328b83e24969bbfa4ce1181cd25e",
            "afd73dc60f9c4bb5b97ac0405c379e6d",
            "25c12f0273e44cc1abe9dd71e9246e18",
            "7cc5eca5941e406b9059c33148b375f6",
            "11a7d50e619d47feacacf9278d2831f8",
            "03dc17ee1dc84914bb8db17b5b68a7f9",
            "22849804229343a9a466fc9926b36fa4",
            "3721d67b91b640ceb3a2d880d66e94ca",
            "424e7d245e6b429a93a492a3bc00cfbd"
          ]
        },
        "outputId": "c13f0fd8-820e-49b8-8114-35d00ddb3728"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c737f7bb0b0e4a049772ffe6d599d46e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ê³ ìˆ˜ì¤€ API(pipeline) ë°©ì‹**\n",
        "\n",
        "ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ pipeline í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ì‰½ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "MGFh99qE35zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "id": "jOOq78Rx35zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab96990-e6d0-4a2e-e161-eb4224675a09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ’¡ **[ì°¸ê³ ] PyTorchì˜ ì–¸ì–´ ëª¨ë¸ ìƒì„± í•¨ìˆ˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\n",
        "\n",
        "| ë§¤ê°œë³€ìˆ˜ | ì˜ë¯¸ | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… |\n",
        "|--- |--- |--- |--- |--- |\n",
        "| **max_new_tokens**  |ëª¨ë¸ì´ ìƒì„±í•  ìƒˆë¡œìš´ í† í°ì˜ ìµœëŒ€ ê°œìˆ˜  |ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¦„ (ë³´í†µ 50-100)  |1 ~ ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´  |  |\n",
        "|**temperature**  |ëª¨ë¸ ì¶œë ¥ì˜ ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì„ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜  |1.0  |0.0 ~ 2.0 (ì¼ë°˜ì ìœ¼ë¡œ 0.1 ~ 1.5)  |Softmax í•¨ìˆ˜ì˜ ì˜¨ë„ ë§¤ê°œë³€ìˆ˜  |\n",
        "|**do_sample**  |ìƒ˜í”Œë§ ë°©ì‹ ì„ íƒ (í™•ë¥ ì  vs ê²°ì •ì )  |False  |Boolean (True/False)  |  |"
      ],
      "metadata": {
        "id": "4clCIvrf_qbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------\n",
        "# max_new_tokens\n",
        "#--------------------------\n",
        "# ì§§ì€ ë‹µë³€ (ìš”ì•½, ë¶„ë¥˜)\n",
        "max_new_tokens=50\n",
        "\n",
        "# ì¤‘ê°„ ê¸¸ì´ ë‹µë³€ (ì„¤ëª…, Q&A)\n",
        "max_new_tokens=200\n",
        "\n",
        "# ê¸´ ë‹µë³€ (ì—ì„¸ì´, ë³´ê³ ì„œ)\n",
        "max_new_tokens=500"
      ],
      "metadata": {
        "id": "01YVt_XCA6MQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Temperature** ê°’ì— ë”°ë¥¸ íŠ¹ì„±\n",
        "\n",
        "|Temperature|íŠ¹ì„±|ì‚¬ìš© ìƒí™©|ì˜ˆì‹œ|\n",
        "|--- |--- |--- |---|\n",
        "|0.0 | ì™„ì „íˆ ê²°ì •ì , í•­ìƒ ê°™ì€ ì¶œë ¥ | ì •í™•í•œ ë‹µë³€ í•„ìš” | ìˆ˜í•™ ë¬¸ì œ, ë²ˆì—­ |\n",
        "|0.3-0.5 | ì•ˆì •ì , ì¼ê´€ì„± ë†’ìŒ | ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì„œ, ê¸°ìˆ  ì„¤ëª… | ë³´ê³ ì„œ ì‘ì„± |\n",
        "|0.7 | ê· í˜• ì¡íŒ ì°½ì˜ì„±ê³¼ ì¼ê´€ì„± | ì¼ë°˜ì ì¸ ëŒ€í™”, êµìœ¡ | ì±—ë´‡, Q&A |\n",
        "|1.0 | í‘œì¤€ ì°½ì˜ì„± | ê¸°ë³¸ ì„¤ì • | ê¸°ë³¸ ëŒ€í™”\n",
        "|1.5-2.0 | ë§¤ìš° ì°½ì˜ì , ì˜ˆì¸¡ ë¶ˆê°€ | ì°½ì‘ í™œë™, ë¸Œë ˆì¸ìŠ¤í† ë° | ì†Œì„¤, ì‹œ ì°½ì‘ |"
      ],
      "metadata": {
        "id": "36N_NtELBcBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------\n",
        "# Temperature ì‘ë™ ì›ë¦¬\n",
        "#--------------------------\n",
        "# ìˆ˜í•™ì  í‘œí˜„\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def apply_temperature(logits, temperature):\n",
        "    \"\"\"Temperature scaling ì ìš©\"\"\"\n",
        "    if temperature == 0:\n",
        "        return torch.argmax(logits, dim=-1)\n",
        "    else:\n",
        "        scaled_logits = logits / temperature\n",
        "        probabilities = F.softmax(scaled_logits, dim=-1)\n",
        "        return torch.multinomial(probabilities, 1)\n",
        "\n",
        "# ì˜ˆì‹œ: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ í™•ë¥ \n",
        "original_probs = [0.6, 0.3, 0.1]  # \"ì¢‹ì€\", \"ë‚˜ìœ\", \"ë³´í†µ\"\n",
        "\n",
        "# Temperature = 0.5 (ë” í™•ì‹¤í•œ ì„ íƒ)\n",
        "temp_05_probs = [0.8, 0.15, 0.05]\n",
        "\n",
        "# Temperature = 1.5 (ë” ë‹¤ì–‘í•œ ì„ íƒ)\n",
        "temp_15_probs = [0.45, 0.35, 0.2]"
      ],
      "metadata": {
        "id": "kL8K-xSIBQUi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **do_sample**ì˜µì…˜ ë¹„êµ\n",
        "\n",
        "| do_sample |  ë°©ì‹ | íŠ¹ì§• | ì–¸ì œ ì‚¬ìš©? |\n",
        "|--- |--- |--- |--- |\n",
        "| False | Greedy Decoding | í•­ìƒ ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ì„ íƒ | ì •í™•ì„±ì´ ì¤‘ìš”í•œ ì‘ì—… |\n",
        "| True | Sampling | í™•ë¥ ì— ë”°ë¼ ëœë¤í•˜ê²Œ ì„ íƒ | ì°½ì˜ì„±ì´ í•„ìš”í•œ ì‘ì—… |"
      ],
      "metadata": {
        "id": "j9P_SQq__0Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# do_sample=False (Greedy)\n",
        "ë‹¤ìŒ ë‹¨ì–´ í™•ë¥ : {\"ì¢‹ì€\": 0.6, \"ë‚˜ìœ\": 0.3, \"ë³´í†µ\": 0.1}\n",
        "ì„ íƒ: í•­ìƒ \"ì¢‹ì€\" (ê°€ì¥ ë†’ì€ í™•ë¥ )\n",
        "\n",
        "# do_sample=True (Sampling)\n",
        "ë‹¤ìŒ ë‹¨ì–´ í™•ë¥ : {\"ì¢‹ì€\": 0.6, \"ë‚˜ìœ\": 0.3, \"ë³´í†µ\": 0.1}\n",
        "ì„ íƒ: 60% í™•ë¥ ë¡œ \"ì¢‹ì€\", 30% í™•ë¥ ë¡œ \"ë‚˜ìœ\", 10% í™•ë¥ ë¡œ \"ë³´í†µ\""
      ],
      "metadata": {
        "id": "dj0OspUlrN1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **í”„ë¡¬í”„íŠ¸ ì‘ì„± ë° ì§ˆì˜**"
      ],
      "metadata": {
        "id": "wbFuicMk35zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í”„ë¡¬í”„íŠ¸ (ì‚¬ìš©ì ì…ë ¥ / ì¿¼ë¦¬)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# ì¶œë ¥ ìƒì„±\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "9G-g_BGk35zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f49df8f-72f8-4193-f349-4f51d3f13f04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Why did the chicken join the band? Because it had the drumsticks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.ì˜ˆì œë¡œ QuickTour**"
      ],
      "metadata": {
        "id": "JFOVRwVI8bjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„±**"
      ],
      "metadata": {
        "id": "hLTlimBYRfid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # GPU ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ì±„íŒ… í˜•ì‹ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"please explain the difference between tuple and dictionary.\"}\n",
        "]\n",
        "\n",
        "# í† í°í™” ë° ìƒì„±\n",
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "# input_ids = input_ids.to('cuda')\n",
        "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ìƒì„±\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,  #ëª¨ë¸ì´ ìƒì„±í•  ìƒˆë¡œìš´ í† í°ì˜ ìµœëŒ€ ê°œìˆ˜\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print('-' * 50)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "1z-FreQiRlsQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "52096082425e494886093f0852f88de3",
            "afa5b5fac97f41eeb83dc44d59f08065",
            "4a6ff28efd3b4d5487365ff26364e4ae",
            "4ea9fa7331ea4697a4ebac008992c6b5",
            "e7112bf7f9a94a88be1331a7b0493df3",
            "5fab4049de494f3fa61177e137d61893",
            "e5b6c17f3ff84ba199a889c787211f45",
            "2ee83b8b0bf14abca6a7472028826dae",
            "29edce5e1db94cefa601a600b995fbb9",
            "8b0f99e99e414524a6e93b8faa5e0ab4",
            "c426c26e03734b449eec5aec122ea2d2"
          ]
        },
        "outputId": "a99668ce-6c5f-42bd-ae63-4d056490e1a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52096082425e494886093f0852f88de3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "You are a helpful AI assistant. please explain the difference between tuple and dictionary. \n",
            " I need to create a C# class that models an error object for a data service. It should inherit from a base error class, have properties for error code (string), error message (string), error type (string), and a nullable `DateTime?` for when the error occurred. Please provide XML documentation for each property, and include constructors: a parameterless one, and another that takes an error code, message, type, and occurrence time. Also, ensure it's clear that the class is auto-generated and shouldn't be manually altered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸**"
      ],
      "metadata": {
        "id": "uRwt3o1zSpV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coding_assistant_example():\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\"},\n",
        "        {\"role\": \"user\", \"content\": \"í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì‚°í•˜ëŠ” íš¨ìœ¨ì ì¸ í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\"}\n",
        "    ]\n",
        "\n",
        "    # íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    # ì‘ë‹µ ìƒì„±\n",
        "    print('-' * 50)\n",
        "    response = pipe(messages)\n",
        "    print(response[0]['generated_text'])\n",
        "coding_assistant_example()"
      ],
      "metadata": {
        "id": "Bgv8CP_rSw6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa140b9-7759-425c-f7cb-3f71e36a3471"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "[{'role': 'system', 'content': 'ë‹¹ì‹ ì€ íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.'}, {'role': 'user', 'content': 'í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ê³„ì‚°í•˜ëŠ” íš¨ìœ¨ì ì¸ í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.'}, {'role': 'assistant', 'content': ' í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì€ ì²« ë²ˆì§¸ ë‘ ê°œì˜ ìˆ˜ê°€ 1ì´ê³  ë‘ ë²ˆì§¸ ë²ˆì§¸ ìˆ˜ëŠ” 2ì´ë©°, ê·¸ ì´í›„ ê° ìˆ˜ëŠ” ì´ì „ ë‘ ìˆ˜ì˜ í•©ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤. ë‹¤ìŒì€ ì´ í•¨ìˆ˜ë¥¼ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤:\\n\\n\\n```python\\n\\ndef fibonacci(n):\\n\\n    if n <= 0:\\n\\n        return []\\n\\n    elif n == 1:\\n\\n        return [0]\\n\\n    elif n == 2:\\n\\n        return [0, 1]\\n\\n\\n    fib_sequence = [0, 1]\\n\\n    for i in range(2, n):\\n\\n        next_number = fib_sequence[i-1] + fib_sequence[i-2]\\n\\n        fib_sequence.append(next_number)\\n\\n    return fib_sequence\\n\\n\\n# í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì˜ '}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: êµìœ¡ìš© Q&A ì‹œìŠ¤í…œ**"
      ],
      "metadata": {
        "id": "9qDx6AzEShCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def educational_qa_system():\n",
        "    \"\"\"êµìœ¡ìš© ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ\"\"\"\n",
        "\n",
        "    class Phi3EducationBot:\n",
        "        def __init__(self, model_name=\"microsoft/Phi-3-mini-4k-instruct\"):\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            self.pipe = pipeline(\"text-generation\",\n",
        "                               model=self.model,\n",
        "                               tokenizer=self.tokenizer)\n",
        "\n",
        "        def answer_question(self, question, subject=\"ì»´í“¨í„°ê³µí•™\"):\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"ë‹¹ì‹ ì€ {subject} ì „ë¬¸ ê°•ì‚¬ì…ë‹ˆë‹¤. í•™ìƒë“¤ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ]\n",
        "\n",
        "            response = self.pipe(messages, max_new_tokens=250)\n",
        "            return response[0]['generated_text']\n",
        "\n",
        "    # ì‚¬ìš© ì˜ˆì‹œ\n",
        "    bot = Phi3EducationBot()\n",
        "    answer = bot.answer_question(\"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
        "    print(answer)\n",
        "\n",
        "educational_qa_system()"
      ],
      "metadata": {
        "id": "d7m4784iSMhd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9a9f286db85041fb828573923855240f",
            "7c634f2001a54be4a0976f78ee27d6f9",
            "4ece9c9126e34cf28c70e80da21539ba",
            "1758babdd46a4aad8446ac130eb17824",
            "dbacab70e2e14cdab88437b27ad3bb91",
            "98a8e5f0f3e24f9bb46245dc674f2ff2",
            "595d8354af364b11845361b43022ad17",
            "e6895c046c424c2496c9c3d72bc99189",
            "0dc8e6ee3b0547e7b95c8c9c87b2e168",
            "dd4417144f094dc18178e8b06b266173",
            "7f29356ba5774ebfafe92626651922f3"
          ]
        },
        "outputId": "397f537b-5273-44dc-9015-44cd28d41a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a9f286db85041fb828573923855240f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: ìˆ˜í•™ì  ì¶”ë¡  í…ŒìŠ¤íŠ¸**"
      ],
      "metadata": {
        "id": "-x5CFg5mYDhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ìˆ˜í•™ì  ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "#   ì§ì ‘ ì•Œê³ ë¦¬ì¦˜ ì‚¬ê³ ê°€ ë‚˜ì˜¤ëŠ” ê³¼ì •ì„ ë³¼ ìˆ˜ ìˆì–´ êµìœ¡ì ìœ¼ë¡œ ìœ ìµ(?)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ (ì‚¬ìš©ì ì…ë ¥ / ì¿¼ë¦¬)\n",
        "prompt = \"ì‹œê³—ë°”ëŠ˜ì´ 15ë¶„ ë™ì•ˆ ì–¼ë§ˆë‚˜ ì›€ì§ì¼ê¹Œìš”?\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "c32AwHKIYmPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WVmM3W2w62-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë¹„êµ**"
      ],
      "metadata": {
        "id": "-Zj0GKg86tq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: Phi-3 4K vs 128K ì»¨í…ìŠ¤íŠ¸ ë¹„êµ ì‹¤ìŠµ**\n",
        "**[ì£¼ì˜!]** ëª¨ë¸ í¬ê¸°ê°€ í¬ê¸° ë•Œë¬¸ì— 128K ëª¨ë¸ì€ ì‹¤í–‰ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆë‹¤."
      ],
      "metadata": {
        "id": "IezMGMJIY6Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì‹¤í—˜ í¬ì¸íŠ¸**\n",
        "\n",
        "1. 4K ë²„ì „ (Phi-3-mini-4k)\n",
        "    - 4,096 í† í°ê¹Œì§€ë§Œ ì¸ì‹\n",
        "    - ê¸´ ë¬¸ë§¥(30K í† í°)ì„ ë„£ìœ¼ë©´ ì•ë¶€ë¶„ì€ ì˜ë¼ë²„ë¦¼ â†’ â€œì •ë³´ ì†ì‹¤â€ ë°œìƒ\n",
        "    - ë”°ë¼ì„œ ë‹µë³€ì´ í‹€ë¦¬ê±°ë‚˜ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŒ\n",
        "2. 128K ë²„ì „ (Phi-3-mini-128k)\n",
        "    - ìµœëŒ€ 128,000 í† í°ê¹Œì§€ ì¸ì‹\n",
        "    - ê¸´ ë¬¸ë§¥ ì „ì²´ë¥¼ ìœ ì§€\n",
        "    - ë‹µë³€ì—ì„œ ì •í™•í•œ ë¬¸ì¥ ê°œìˆ˜ë‚˜ ë” ê¹Šì€ ë¬¸ë§¥ ì´í•´ê°€ ê°€ëŠ¥"
      ],
      "metadata": {
        "id": "KAmWuompZN_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 4K ë²„ì „ê³¼ 128K ë²„ì „ ê°ê° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "models = {\n",
        "    \"4k\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    \"128k\": \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "}\n",
        "\n",
        "tokenizers = {}\n",
        "loaded_models = {}\n",
        "\n",
        "for key, model_name in models.items():\n",
        "    tokenizers[key] = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    loaded_models[key] = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, trust_remote_code=True, device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "# ê¸´ í…ìŠ¤íŠ¸ ìƒì„± (í…ŒìŠ¤íŠ¸ìš© ë¬¸ë§¥)\n",
        "long_text = \"ì´ ë¬¸ì¥ì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤. \" * 3000  # ì•½ 30K í† í° ìˆ˜ì¤€\n",
        "\n",
        "prompt = long_text + \"\\n\\nì§ˆë¬¸: ìœ„ ë¬¸ì¥ì˜ ê°œìˆ˜ëŠ” ëª‡ ê°œì¸ê°€ìš”?\"\n",
        "\n",
        "# ë‘ ëª¨ë¸ì— ê°ê° ì…ë ¥\n",
        "for key in [\"4k\", \"128k\"]:\n",
        "    tokenizer = tokenizers[key]\n",
        "    model = loaded_models[key]\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    print(f\"\\n===== Phi-3 {key.upper()} ê²°ê³¼ =====\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "wgA_FG5IZFYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0g36MizoMox8"
      }
    }
  ]
}